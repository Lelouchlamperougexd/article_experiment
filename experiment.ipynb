{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5S-uaIyCZ_D",
        "outputId": "ad74a4cc-1693-4e03-fd8e-b3df282da925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "# RAG Experiment with Wikipedia and ArXiv datasets\n",
        "\n",
        "# Install required packages\n",
        "!pip install datasets pandas numpy torch tqdm faiss-cpu sentence-transformers transformers openpyxl\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Loading and preparing data\n",
        "# ==============================\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Loading Wikipedia-Articles dataset\n",
        "try:\n",
        "    dataset_wiki = load_dataset(\"BrightData/Wikipedia-Articles\", split=\"train\")\n",
        "    docs_wiki_df = pd.DataFrame(dataset_wiki)\n",
        "    # If the dataset doesn't have an explicit identifier, create one by index\n",
        "    if 'doc_id' not in docs_wiki_df.columns:\n",
        "        docs_wiki_df['doc_id'] = \"wiki_\" + docs_wiki_df.index.astype(str)\n",
        "    # Process document text (concatenating title and text)\n",
        "    docs_wiki_df['full_text'] = docs_wiki_df['title'] + \". \" + docs_wiki_df['cataloged_text'].fillna(\"\")\n",
        "    print(f\"Loaded {len(docs_wiki_df)} documents from Wikipedia\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Wikipedia-Articles: {e}\")\n",
        "    docs_wiki_df = pd.DataFrame()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "Nb732jXlC7KL",
        "outputId": "79b9ece4-4790-4a66-a84e-9dc5053e4bbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Error loading Wikipedia-Articles: name 'load_dataset' is not defined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f78ba222c8a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdataset_wiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BrightData/Wikipedia-Articles\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdocs_wiki_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_wiki\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f78ba222c8a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error loading Wikipedia-Articles: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdocs_wiki_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading ArXiv dataset\n",
        "try:\n",
        "    dataset_arxiv = load_dataset(\"arxiv_dataset\", split=\"train[:1000]\")  # Limiting for example\n",
        "    docs_arxiv_df = pd.DataFrame(dataset_arxiv)\n",
        "    # Create identifier\n",
        "    if 'doc_id' not in docs_arxiv_df.columns:\n",
        "        docs_arxiv_df['doc_id'] = \"arxiv_\" + docs_arxiv_df.index.astype(str)\n",
        "    # Process document text\n",
        "    if 'abstract' in docs_arxiv_df.columns and 'title' in docs_arxiv_df.columns:\n",
        "        docs_arxiv_df['full_text'] = docs_arxiv_df['title'] + \". \" + docs_arxiv_df['abstract'].fillna(\"\")\n",
        "    print(f\"Loaded {len(docs_arxiv_df)} documents from ArXiv\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ArXiv: {e}\")\n",
        "    # Create sample ArXiv data if failed to load the dataset\n",
        "    docs_arxiv_df = pd.DataFrame({\n",
        "        'doc_id': [f\"arxiv_{i}\" for i in range(10)],\n",
        "        'title': [f\"ArXiv Paper {i}\" for i in range(10)],\n",
        "        'abstract': [f\"Abstract of paper {i} discussing scientific topics.\" for i in range(10)],\n",
        "        'full_text': [f\"ArXiv Paper {i}. Abstract of paper {i} discussing scientific topics.\" for i in range(10)]\n",
        "    })\n",
        "\n",
        "# Merge datasets (optional)\n",
        "all_docs_df = pd.concat([docs_wiki_df, docs_arxiv_df], ignore_index=True)\n",
        "print(f\"Total documents: {len(all_docs_df)}\")"
      ],
      "metadata": {
        "id": "qdPI8ms9chTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create query set for experiment\n",
        "queries = [\n",
        "    {\"query_id\": \"q1\", \"query_text\": \"History of Wikipedia\", \"dataset\": \"wiki\"},\n",
        "    {\"query_id\": \"q2\", \"query_text\": \"Climate change\", \"dataset\": \"both\"},\n",
        "    {\"query_id\": \"q3\", \"query_text\": \"Artificial intelligence and machine learning\", \"dataset\": \"both\"},\n",
        "    {\"query_id\": \"q4\", \"query_text\": \"Quantum mechanics\", \"dataset\": \"arxiv\"},\n",
        "    {\"query_id\": \"q5\", \"query_text\": \"Deep learning in computer vision\", \"dataset\": \"arxiv\"},\n",
        "]\n",
        "queries_df = pd.DataFrame(queries)\n",
        "print(\"Query set:\")\n",
        "print(queries_df)"
      ],
      "metadata": {
        "id": "F-yitAc1HNQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Retrieval Components\n",
        "# ======================\n",
        "\n",
        "# Path to save indexes\n",
        "INDEX_DIR = \"./search_indexes\"\n",
        "os.makedirs(INDEX_DIR, exist_ok=True)\n",
        "\n",
        "# Function to create a simple inverted index without pyserini\n",
        "def create_simple_index(documents_df):\n",
        "    \"\"\"Creates a simple inverted index for BM25-like search\"\"\"\n",
        "    print(\"Creating simple BM25-like index...\")\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(documents_df['full_text'])\n",
        "\n",
        "    # Create dictionaries for search\n",
        "    vocabulary = vectorizer.vocabulary_\n",
        "    idf = vectorizer.idf_\n",
        "    doc_vectors = X\n",
        "\n",
        "    return {\n",
        "        'vectorizer': vectorizer,\n",
        "        'vocabulary': vocabulary,\n",
        "        'idf': idf,\n",
        "        'doc_vectors': doc_vectors,\n",
        "        'document_ids': documents_df['doc_id'].tolist()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZK-9ojVmHYDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create indexes for search\n",
        "# For BM25 we use a simple index instead of pyserini\n",
        "bm25_index = create_simple_index(all_docs_df)\n",
        "\n",
        "# Function to search using simple index\n",
        "def simple_bm25_search(query, index, top_k=10):\n",
        "    vectorizer = index['vectorizer']\n",
        "    doc_vectors = index['doc_vectors']\n",
        "    document_ids = index['document_ids']\n",
        "\n",
        "    # Vectorize query\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    # Calculate similarity\n",
        "    similarity_scores = query_vector.dot(doc_vectors.T).toarray()[0]\n",
        "\n",
        "    # Sort results\n",
        "    top_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
        "    results = [(document_ids[idx], similarity_scores[idx]) for idx in top_indices]\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function for BM25 search\n",
        "def bm25_search(query, top_k=10):\n",
        "    # Use simple index instead of pyserini\n",
        "    return simple_bm25_search(query, bm25_index, top_k)\n"
      ],
      "metadata": {
        "id": "KW1rnzYKHZ1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense search with SentenceTransformer and FAISS\n",
        "print(\"Creating vector index...\")\n",
        "# Load pre-trained model\n",
        "model_name = 'sentence-transformers/all-mpnet-base-v2'  # Better quality, moderate speed\n",
        "try:\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Create list of document texts for encoding\n",
        "    doc_texts = all_docs_df['full_text'].tolist()\n",
        "\n",
        "    # Encode documents into vector representation (using batches to save memory)\n",
        "    batch_size = 64\n",
        "    doc_embeddings = None\n",
        "    for i in tqdm(range(0, len(doc_texts), batch_size), desc=\"Vectorizing documents\"):\n",
        "        batch_texts = doc_texts[i:i+batch_size]\n",
        "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
        "        batch_embeddings_np = batch_embeddings.cpu().numpy()\n",
        "\n",
        "        if doc_embeddings is None:\n",
        "            doc_embeddings = batch_embeddings_np\n",
        "        else:\n",
        "            doc_embeddings = np.vstack((doc_embeddings, batch_embeddings_np))\n",
        "\n",
        "    # Create FAISS index for vector search\n",
        "    dimension = doc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    # Normalize vectors for cosine similarity\n",
        "    faiss.normalize_L2(doc_embeddings)\n",
        "    index.add(doc_embeddings)\n",
        "\n",
        "    # Save objects for search function\n",
        "    dense_search_objects = {\n",
        "        'model': model,\n",
        "        'index': index,\n",
        "        'doc_ids': all_docs_df['doc_id'].tolist()\n",
        "    }\n",
        "\n",
        "    def dense_search(query, top_k=10):\n",
        "        # Encode query\n",
        "        q_emb = dense_search_objects['model'].encode([query], convert_to_tensor=True)\n",
        "        q_emb = q_emb.cpu().numpy()\n",
        "        # Normalize for cosine similarity\n",
        "        faiss.normalize_L2(q_emb)\n",
        "        # Search\n",
        "        scores, indices = dense_search_objects['index'].search(q_emb, k=top_k)\n",
        "        # Format results list\n",
        "        results = []\n",
        "        for idx, score in zip(indices[0], scores[0]):\n",
        "            if idx < len(dense_search_objects['doc_ids']):\n",
        "                doc_id = dense_search_objects['doc_ids'][idx]\n",
        "                results.append((doc_id, float(score)))\n",
        "        return results\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error creating vector index: {e}\")\n",
        "    # Create fallback if index creation failed\n",
        "    def dense_search(query, top_k=10):\n",
        "        print(\"Dense search failed to initialize. Returning empty results.\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "rE8FdQAFHdsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid search\n",
        "def normalize_scores(results):\n",
        "    \"\"\"Normalizes scores to range [0, 1]\"\"\"\n",
        "    if not results:\n",
        "        return {}\n",
        "\n",
        "    docs_scores = {doc_id: score for doc_id, score in results}\n",
        "    scores = list(docs_scores.values())\n",
        "\n",
        "    min_score, max_score = min(scores), max(scores)\n",
        "    if max_score == min_score:\n",
        "        return {doc_id: 1.0 for doc_id in docs_scores}\n",
        "\n",
        "    norm_scores = {\n",
        "        doc_id: (score - min_score) / (max_score - min_score)\n",
        "        for doc_id, score in docs_scores.items()\n",
        "    }\n",
        "    return norm_scores\n",
        "\n",
        "def hybrid_search(query, lambda_weight=0.5, top_k=10):\n",
        "    \"\"\"Combines results from BM25 and dense search\"\"\"\n",
        "    # Get results from both search methods\n",
        "    bm25_results = bm25_search(query, top_k=top_k*2)  # Get more to have something to combine\n",
        "    dense_results = dense_search(query, top_k=top_k*2)\n",
        "\n",
        "    # Normalize scores\n",
        "    bm25_norm = normalize_scores(bm25_results)\n",
        "    dense_norm = normalize_scores(dense_results)\n",
        "\n",
        "    # Combine document IDs from both methods\n",
        "    all_docs = set(bm25_norm.keys()).union(set(dense_norm.keys()))\n",
        "\n",
        "    # Calculate hybrid scores\n",
        "    hybrid_scores = {}\n",
        "    for doc in all_docs:\n",
        "        score_bm25 = bm25_norm.get(doc, 0)\n",
        "        score_dense = dense_norm.get(doc, 0)\n",
        "        hybrid_scores[doc] = lambda_weight * score_bm25 + (1 - lambda_weight) * score_dense\n",
        "\n",
        "    # Sort by combined score\n",
        "    hybrid_results = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return hybrid_results[:top_k]\n"
      ],
      "metadata": {
        "id": "oQCPb_TQHqBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading generative model...\")\n",
        "try:\n",
        "    # Use small FLAN-T5 model for generation\n",
        "    model_name = \"google/flan-t5-small\"  # Small model, fast, works on CPU\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    # Create pipeline for generation\n",
        "    generator = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    def generate_answer(query, contexts, max_context_length=1500):\n",
        "        \"\"\"Generates answer based on query and contexts\"\"\"\n",
        "        # Limit context length for model\n",
        "        combined_context = \" \".join(contexts)\n",
        "        if len(combined_context) > max_context_length:\n",
        "            combined_context = combined_context[:max_context_length]\n",
        "\n",
        "        # Form prompt for generative model\n",
        "        prompt = f\"\"\"\n",
        "Answer the question based on the given context.\n",
        "\n",
        "Context: {combined_context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Generate answer\n",
        "        output = generator(prompt, max_length=150, num_return_sequences=1)\n",
        "        return output[0]['generated_text']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading generative model: {e}\")\n",
        "    # Create fallback if model loading failed\n",
        "    def generate_answer(query, contexts, max_context_length=1500):\n",
        "        return \"Failed to load generative model. Here are the contexts that were found: \" + \" [...] \".join(contexts[:3])\n"
      ],
      "metadata": {
        "id": "9s1rLL6ZIVCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Complete RAG Pipeline\n",
        "# =====================\n",
        "\n",
        "def rag_pipeline(query, search_method=\"hybrid\", num_docs=3):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline:\n",
        "    1. Find relevant documents\n",
        "    2. Extract contexts\n",
        "    3. Generate answer\n",
        "\n",
        "    Args:\n",
        "        query: query text\n",
        "        search_method: search method ('bm25', 'dense', or 'hybrid')\n",
        "        num_docs: number of documents to use in context\n",
        "    \"\"\"\n",
        "    # 1. Get search results based on chosen method\n",
        "    if search_method == \"bm25\":\n",
        "        search_results = bm25_search(query, top_k=num_docs)\n",
        "    elif search_method == \"dense\":\n",
        "        search_results = dense_search(query, top_k=num_docs)\n",
        "    else:  # hybrid\n",
        "        search_results = hybrid_search(query, top_k=num_docs)\n",
        "\n",
        "    # 2. Extract document texts\n",
        "    doc_ids = [doc_id for doc_id, _ in search_results]\n",
        "    contexts = []\n",
        "    for doc_id in doc_ids:\n",
        "        # Find document in DataFrame\n",
        "        doc_row = all_docs_df[all_docs_df['doc_id'] == doc_id]\n",
        "        if not doc_row.empty:\n",
        "            contexts.append(doc_row['full_text'].values[0])\n",
        "\n",
        "    # 3. Generate answer\n",
        "    answer = generate_answer(query, contexts)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"search_results\": search_results,\n",
        "        \"contexts\": contexts,\n",
        "        \"answer\": answer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "KHHlDURVIYGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Results Evaluation\n",
        "# ===================\n",
        "\n",
        "# Function to save results for subsequent evaluation\n",
        "def save_results_for_evaluation(results, output_file=\"rag_evaluation_results.json\"):\n",
        "    \"\"\"Saves results for subsequent expert evaluation\"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Results saved to {output_file}\")\n",
        "\n",
        "    # For Google Colab - also download the file\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(output_file)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Run experiment with different search methods\n",
        "def run_experiment():\n",
        "    all_results = []\n",
        "\n",
        "    methods = [\"bm25\", \"dense\", \"hybrid\"]\n",
        "\n",
        "    # Process all queries\n",
        "    for _, query_row in queries_df.iterrows():\n",
        "        query_id = query_row['query_id']\n",
        "        query_text = query_row['query_text']\n",
        "\n",
        "        query_results = {\n",
        "            \"query_id\": query_id,\n",
        "            \"query_text\": query_text,\n",
        "            \"methods\": {}\n",
        "        }\n",
        "\n",
        "        # Execute RAG with each search method\n",
        "        for method in methods:\n",
        "            print(f\"Executing query {query_id} with method {method}...\")\n",
        "            try:\n",
        "                result = rag_pipeline(query_text, search_method=method)\n",
        "                query_results[\"methods\"][method] = {\n",
        "                    \"search_results\": [\n",
        "                        {\"doc_id\": doc_id, \"score\": float(score)}\n",
        "                        for doc_id, score in result[\"search_results\"]\n",
        "                    ],\n",
        "                    \"answer\": result[\"answer\"]\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Error executing RAG with method {method}: {e}\")\n",
        "                query_results[\"methods\"][method] = {\n",
        "                    \"search_results\": [],\n",
        "                    \"answer\": f\"Error: {str(e)}\"\n",
        "                }\n",
        "\n",
        "        all_results.append(query_results)\n",
        "\n",
        "    # Save results\n",
        "    save_results_for_evaluation(all_results)\n",
        "\n",
        "    # Display brief information about results\n",
        "    print(\"\\nExperiment results:\")\n",
        "    for result in all_results:\n",
        "        print(f\"\\nQuery: {result['query_text']}\")\n",
        "        for method, data in result[\"methods\"].items():\n",
        "            print(f\"  Method: {method}\")\n",
        "            print(f\"  Documents found: {len(data['search_results'])}\")\n",
        "            print(f\"  First 100 characters of answer: {data['answer'][:100]}...\")\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "WMeayQUGIads"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create template for expert evaluation\n",
        "# ======================================\n",
        "\n",
        "def create_evaluation_template(results, output_file=\"expert_evaluation_template.xlsx\"):\n",
        "    \"\"\"Creates Excel template for expert evaluation\"\"\"\n",
        "    try:\n",
        "        evaluation_data = []\n",
        "\n",
        "        for result in results:\n",
        "            query_id = result[\"query_id\"]\n",
        "            query_text = result[\"query_text\"]\n",
        "\n",
        "            for method, data in result[\"methods\"].items():\n",
        "                # Add rows for answer evaluation\n",
        "                evaluation_data.append({\n",
        "                    \"query_id\": query_id,\n",
        "                    \"query_text\": query_text,\n",
        "                    \"method\": method,\n",
        "                    \"doc_id\": \"N/A\",  # For answer evaluation\n",
        "                    \"title\": \"ANSWER\",\n",
        "                    \"text\": data[\"answer\"],\n",
        "                    \"relevance_score\": \"\",  # For expert to fill in\n",
        "                    \"comments\": \"\"  # For expert to fill in\n",
        "                })\n",
        "\n",
        "                # Add rows for found document evaluation\n",
        "                for i, doc_info in enumerate(data[\"search_results\"]):\n",
        "                    doc_id = doc_info[\"doc_id\"]\n",
        "                    doc_row = all_docs_df[all_docs_df['doc_id'] == doc_id]\n",
        "\n",
        "                    if not doc_row.empty:\n",
        "                        title = doc_row['title'].values[0] if 'title' in doc_row.columns else \"No title\"\n",
        "                        text = doc_row['full_text'].values[0][:500] + \"...\"  # Trim for readability\n",
        "\n",
        "                        evaluation_data.append({\n",
        "                            \"query_id\": query_id,\n",
        "                            \"query_text\": query_text,\n",
        "                            \"method\": method,\n",
        "                            \"doc_id\": doc_id,\n",
        "                            \"title\": title,\n",
        "                            \"text\": text,\n",
        "                            \"relevance_score\": \"\",  # For expert to fill in\n",
        "                            \"comments\": \"\"  # For expert to fill in\n",
        "                        })\n",
        "\n",
        "        # Create DataFrame and save to Excel\n",
        "        eval_df = pd.DataFrame(evaluation_data)\n",
        "        eval_df.to_excel(output_file, index=False)\n",
        "        print(f\"Expert evaluation template created: {output_file}\")\n",
        "\n",
        "        # For Google Colab - also download the file\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(output_file)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating evaluation template: {e}\")\n"
      ],
      "metadata": {
        "id": "HiSsf7gqIcSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Run the experiment\n",
        "# ====================\n",
        "\n",
        "# Run the experiment\n",
        "print(\"Starting RAG experiment...\")\n",
        "results = run_experiment()\n",
        "create_evaluation_template(results)\n",
        "print(\"Experiment completed!\")"
      ],
      "metadata": {
        "id": "mDjJB_-tIeCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Custom test function\n",
        "# =====================\n",
        "\n",
        "def test_rag_with_custom_query(query, search_method=\"hybrid\"):\n",
        "    \"\"\"Test RAG with a custom query to see how it performs\"\"\"\n",
        "    print(f\"\\nTesting with custom query: '{query}'\")\n",
        "    print(f\"Using search method: {search_method}\")\n",
        "\n",
        "    result = rag_pipeline(query, search_method=search_method)\n",
        "\n",
        "    print(\"\\nSearch results:\")\n",
        "    for i, (doc_id, score) in enumerate(result[\"search_results\"]):\n",
        "        doc_row = all_docs_df[all_docs_df['doc_id'] == doc_id]\n",
        "        if not doc_row.empty:\n",
        "            title = doc_row['title'].values[0] if 'title' in doc_row.columns else \"No title\"\n",
        "            print(f\"{i+1}. {title} (score: {score:.4f})\")\n",
        "\n",
        "    print(\"\\nGenerated answer:\")\n",
        "    print(result[\"answer\"])\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example custom query test\n",
        "test_rag_with_custom_query(\"What are the benefits of renewable energy?\")"
      ],
      "metadata": {
        "id": "fgP4VD9bIf6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}